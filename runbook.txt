PREREQUISITES:
Nix
Python 3.11 or higher 

To set up the tooling environment (in a local context) and run the data pipelines:

 - clone the repo
 - issue "nix-shell" to install the system dependencies
 - issue 
    "poetry install"

    and then

    "poetry shell" 
    
    to install the Python dependencies and start the virtual environment.

 - set the necessary environment vars: 

	PYTHONPATH must include the repository root directory. ( export PYTHONPATH=`pwd` )

Then, at the shell prompt, issue:

 make <pipeline-name>

 where <pipeline-name> is the name of a make target, to run the code.

 The make targets are divided into Utility, (SQL) script-generation, and pipeline targets.
 Run the init-dirs target first to create all the temporary directories the project will need.

 Then issue 

 make db-up 

 to spin up both the MongoDB document store and the PostgreSQL OLAP database.

 Mongo requires no prepopulation, but the OLAP db must be set up before running any pipelines.

issue "make db-init" and "make db-prepopulate" to do this.
 
You can verify the existence of the required tables by issuing "make dbalogin" and inspecting the db.

Issue "make pipeline-ingest" to run the pipeline end-to-end.


++++++++++++++++++++++++++++++++++
+
+ Makefile instrumentation: "makeblocks"

The Makefile may contain some comments which contain a particular syntax of the form:

+open-varblock(...)
+open-cmdblock(...)

and so on. These comments are part of the data pipeline testing and instrumentation system. Do not alter or remove them
unless you know what you are doing.